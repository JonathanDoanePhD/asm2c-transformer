# Placeholder tokenizer script in case you want to implement custom tokenization
def basic_tokenize(text):
    return text.strip().split()